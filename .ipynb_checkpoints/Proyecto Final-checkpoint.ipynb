{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F9zIhQvm0GHP"
   },
   "source": [
    "# <center>Proyecto Final - Procesos Estocásticos</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iwHgczxx0GHk"
   },
   "source": [
    "- Juan Pablo Barrera Avella\n",
    "\n",
    "- Natalia Coy Lozano\n",
    "\n",
    "- Julieth Andrea López Castiblanco"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Bj2G_Bjv0GHo"
   },
   "source": [
    "The purpose of this notebook is to show how to do Part - Of - Speech Tagging using Hidden Markov Models. The theoterical background of the model was already discuss in the attached PDF file and, thus, it is skipped in this notebook as well as the methodology which has already been described. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J-_-AkPy0GHq"
   },
   "source": [
    "Initially, we import the required libraries. Please, note that nltk needs to be installed as it is not included in Python by default. From this library we extract the Hindi corpus of words and we use the method of Naive Bayes which is already built in it as the main purpose of this notebook is to show hot to use HMM in NLP, not how to use the Naive Bayes classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 816,
     "status": "ok",
     "timestamp": 1607522749763,
     "user": {
      "displayName": "Julieth Lopez Castiblanco",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiXgRGFVQaqVYfXgy3uMbAXCvYRF_rCyiH0tA6B7w=s64",
      "userId": "16963728174304468671"
     },
     "user_tz": 300
    },
    "id": "YMCu-RZ-0GHr"
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "import numpy as np\n",
    "from nltk.corpus import indian\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "txXUT0Bu0GHw"
   },
   "source": [
    "We load the Indian Corpus, which is a matrix of data, containing in each row a sentence such that each position of the matrix contains a tuple of the word and the real tag of the word. Let´s see an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 860,
     "status": "error",
     "timestamp": 1607522754681,
     "user": {
      "displayName": "Julieth Lopez Castiblanco",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiXgRGFVQaqVYfXgy3uMbAXCvYRF_rCyiH0tA6B7w=s64",
      "userId": "16963728174304468671"
     },
     "user_tz": 300
    },
    "id": "LqzkAJ9M0GHz",
    "outputId": "47b5b9fc-721a-45cb-dded-146f46073d3b"
   },
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/nltk/corpus/util.py\u001b[0m in \u001b[0;36m__load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     79\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 80\u001b[0;31m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mroot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'{}/{}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubdir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mzip_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     81\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/nltk/data.py\u001b[0m in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    672\u001b[0m     \u001b[0mresource_not_found\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'\\n%s\\n%s\\n%s\\n'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0msep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 673\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresource_not_found\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    674\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mindian\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('indian')\n  \u001b[0m\n  Searched in:\n    - '/root/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n    - '/usr/nltk_data'\n    - '/usr/lib/nltk_data'\n**********************************************************************\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-513523121241>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindian\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtagged_sents\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'hindi.pos'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mtrain_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/nltk/corpus/util.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, attr)\u001b[0m\n\u001b[1;32m    114\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"LazyCorpusLoader object has no attribute '__bases__'\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m         \u001b[0;31m# This looks circular, but its not, since __load() changes our\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m         \u001b[0;31m# __class__ to something new:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/nltk/corpus/util.py\u001b[0m in \u001b[0;36m__load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     79\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mroot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'{}/{}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubdir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mzip_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m                 \u001b[0;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m         \u001b[0;31m# Load the corpus.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/nltk/corpus/util.py\u001b[0m in \u001b[0;36m__load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     76\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 78\u001b[0;31m                 \u001b[0mroot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'{}/{}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubdir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     79\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mroot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'{}/{}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubdir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mzip_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/nltk/data.py\u001b[0m in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    671\u001b[0m     \u001b[0msep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'*'\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m70\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    672\u001b[0m     \u001b[0mresource_not_found\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'\\n%s\\n%s\\n%s\\n'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0msep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 673\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresource_not_found\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    674\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    675\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mindian\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('indian')\n  \u001b[0m\n  Searched in:\n    - '/root/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n    - '/usr/nltk_data'\n    - '/usr/lib/nltk_data'\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "train_data = indian.tagged_sents('hindi.pos')\n",
    "train_data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F_iYT7kL0GH7"
   },
   "source": [
    "As preprocessing, we are going to turn all the sentences into a database of words (with its correspondent tag label):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IPxddtOy0GH8",
    "outputId": "a3be4829-f163-4fd3-a193-99e6ee4b11d1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('पूर्ण', 'JJ')"
      ]
     },
     "execution_count": 3,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_of_hindi = []\n",
    "for i in train_data:\n",
    "    for j in range(len(i)):\n",
    "        if (i[j][1]!=\"\"):\n",
    "            list_of_hindi.append((i[j][0],i[j][1]))\n",
    "list_of_hindi[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pLysXHkq0GH-"
   },
   "source": [
    "## Naive Bayes Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P-kyhmFR0GIA"
   },
   "source": [
    "To use the classifier, we first need to create features to obtain onformation from. In this sense, we propose the next features:\n",
    "1. The first and the last letter.\n",
    "2. The first 2 and the last 2 letters.\n",
    "3. The first 3 and the last 3 letters.\n",
    "4. The first 4 and the last 4 letters.\n",
    "5. the length of the word.\n",
    "6. The times each letter appears in each word.\n",
    "\n",
    "The description in 1 to 4 corresponds to prefixes and sufixes (possibly). The length of the word as well as the appereance of letters might also add some value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NnSxix7A0GIE"
   },
   "outputs": [],
   "source": [
    "def create_features(word):\n",
    "    features = {}\n",
    "    features[\"first_letter\"] = word[0]\n",
    "    features[\"last_letter\"] = word[-1]\n",
    "    features[\"first_2_letter\"] = word[0:2]\n",
    "    features[\"last_2_letter\"] = word[len(word)-2:]\n",
    "    features[\"first_3_letter\"] = word[0:3]\n",
    "    features[\"last_3_letter\"] = word[len(word)-3:]\n",
    "    features[\"first_4_letter\"] = word[0:4]\n",
    "    features[\"last_4_letter\"] = word[len(word)-4:]\n",
    "    features[\"whole_word\"] = word\n",
    "    features[\"len\"] = len(word)\n",
    "    for letter in 'अआएईऍऎऐइओऑऒऊऔउबभचछडढफफ़गघग़हजझकखख़लळऌऴॡमनङञणऩॐपक़रऋॠऱसशषटतठदथधड़ढ़वयय़ज़|,;?!-”“':\n",
    "        features[\"count({})\".format(letter)] = word.lower().count(letter)\n",
    "        #features[\"has({})\".format(letter)] = (letter in word.lower())\n",
    "    return features\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6JETUVM_0GIF"
   },
   "source": [
    "To train the model, we split the dataset randomly into train and test set. Finally, we train the Naive Bayes classifier and assess its performance with the accuracy of the model. The result is shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ga1NAhRc0GIF"
   },
   "outputs": [],
   "source": [
    "random.shuffle(list_of_hindi)\n",
    "featuresets = [(create_features(n), tag) for (n, tag) in list_of_hindi]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "p22n0Qs90GIG",
    "outputId": "60617cc0-0691-4f28-deeb-3224bc4a4edc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7913333333333333\n"
     ]
    }
   ],
   "source": [
    "train_set, test_set = featuresets[3000:], featuresets[:3000]\n",
    "classifier = nltk.NaiveBayesClassifier.train(train_set)\n",
    "print(nltk.classify.accuracy(classifier, test_set))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8dFZaemg0GII"
   },
   "source": [
    "As it has approximately 80% of accuracy, we consider it has a good performance, taking into account the lack of data. Now, to have a better  model, we will train it on the whole dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "snXZL5En0GIJ"
   },
   "outputs": [],
   "source": [
    "classifier = nltk.NaiveBayesClassifier.train(featuresets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CZjb7k4o0GIK"
   },
   "source": [
    "When we put some words in hindi, i.e., विश्वनाथन, छात्, etc. we obtain the next result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wejIkClt0GIK",
    "outputId": "4a72437e-10a8-4a10-90b9-4bb573732a4a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NNPC\n",
      "NN\n",
      "NVB\n"
     ]
    }
   ],
   "source": [
    "print(classifier.classify(create_features(\"विश्वनाथन\"))) #Viswanathan\n",
    "print(classifier.classify(create_features(\"छात्रों\"))) #Estudiante\n",
    "print(classifier.classify(create_features(\"मैंने सोचा\"))) #Pensé"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eYHvb7kq0GIL"
   },
   "source": [
    "## Viterbi Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "K9PGiPxg0GIM"
   },
   "outputs": [],
   "source": [
    "dict_of_tags={\"NN\":0,\"JJ\":0,\"VFM\":0,\"SYM\":0,\"NNP\":0,\"NNC\":0,\"INTF\":0,\"CC\":0,\"PREP\":0,\"PRP\":0,\"NVB\":0,\"VAUX\":0,\"PUNC\":0,\"NNPC\":0,\"VRB\":0,\"VNN\":0,\n",
    "\"RB\":0,\"QFNUM\":0,\"RP\":0,\"NEG\":0,\"QF\":0,\"JVB\":0,\"NLOC\":0,\"VJJ\":0,\"QW\":0,\"VM\":0,\"JJC\":0,\"PSP\":0,\"NST\":0,\"QC\":0,\"DEM\":0,\"WQ\":0,\"QO\":0,\"RDP\":0,\"\":0,}\n",
    "dict_for_matrix={\"NN\":0,\"JJ\":1,\"VFM\":2,\"SYM\":3,\"NNP\":4,\"NNC\":5,\"INTF\":6,\"CC\":7,\"PREP\":8,\"PRP\":9,\"NVB\":10,\"VAUX\":11,\"PUNC\":12,\"NNPC\":13,\"VRB\":14,\n",
    "\"VNN\":15,\"RB\":16,\"QFNUM\":17,\"RP\":18,\"NEG\":19,\"QF\":20,\"JVB\":21,\"NLOC\":22,\"VJJ\":23,\"QW\":24,\"VM\":25,\"JJC\":26,\"PSP\":27,\"NST\":28,\"QC\":29,\"DEM\":30,\n",
    "\"WQ\":31,\"QO\":32,\"RDP\":33,\"\":34}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1-gwKv4c0GIN"
   },
   "outputs": [],
   "source": [
    "example_corpus=[[(\"सुरज\",\"NN\"),(\"उगता\",\"VRB\"),(\"है\",\"VAUX\"),(\"|\",\"PUNC\"),(\"मेरा\",\"PRP\"),(\"नाम\",\"NN\"),(\"सुरज\",\"NN\"),(\"है\",\"VAUX\"),( \"|\",\"PUNC\"),(\"हम\",\"PRP\"),(\"चलते\",\"VRB\"),(\"है\",\"VAUX\"),(\"|\",\"PUNC\")]]\n",
    "def initial_state(corpus,tag_dict):\n",
    "    for i in corpus:\n",
    "        for j in range(len(i)):\n",
    "            tag_dict[i[j][1]]=tag_dict[i[j][1]]+1\n",
    "    counter=0\n",
    "    initial_state=np.zeros(len(tag_dict))\n",
    "    for i in tag_dict:\n",
    "        initial_state[counter]=tag_dict[i]\n",
    "        counter+=1\n",
    "    num_words= 0\n",
    "    frecuency=initial_state\n",
    "    for i in corpus:\n",
    "        num_words+=len(i)\n",
    "    return initial_state/num_words,frecuency\n",
    "def transition_matrix(corpus,tag_dict=dict_for_matrix):\n",
    "    count_of_tag=np.zeros(len(dict_for_matrix))\n",
    "    for i in corpus:\n",
    "        for j in range(len(i)-1):\n",
    "            count_of_tag[tag_dict[i[j][1]]]+=1\n",
    "    transition_matrix = np.zeros((len(dict_for_matrix),len(dict_for_matrix)))\n",
    "    for i in corpus:\n",
    "        for j in range(len(i)-1):\n",
    "            transition_matrix[tag_dict[i[j][1]]][tag_dict[i[j+1][1]]]+=1/count_of_tag[tag_dict[i[j][1]]]\n",
    "    return transition_matrix,count_of_tag\n",
    "def emission_matrix(words,corpus,frecuency_of_words,tag_dict=dict_for_matrix):\n",
    "    emission_matrix = np.zeros((len(np.unique(words)),len(dict_for_matrix)))\n",
    "    counter=0\n",
    "    for i in np.unique(words):\n",
    "        for j in corpus:\n",
    "            for k in range(len(j)):\n",
    "                if(j[k][0]==i):\n",
    "                    emission_matrix[counter][tag_dict[j[k][1]]]+=1/frecuency_of_words[tag_dict[j[k][1]]]\n",
    "        counter+=1\n",
    "    return emission_matrix    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KmEOlYSJ0GIN"
   },
   "outputs": [],
   "source": [
    "hindi_words = nltk.corpus.indian.words('hindi.pos')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "prQeB-6G0GIO"
   },
   "outputs": [],
   "source": [
    "initial_state,frecuency_of_words=initial_state(train_data,dict_of_tags)\n",
    "transition_matrix,frecuency_of_tag=transition_matrix(train_data)\n",
    "emission_matrix=emission_matrix(hindi_words,train_data,frecuency_of_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H16pyPkG0GIQ"
   },
   "source": [
    "We have some classes that the data haven´t seen yet, so we remove them as what we are obtaining is not a transition matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MqPHiHZ80GIQ"
   },
   "outputs": [],
   "source": [
    "transition_matrix=np.delete(transition_matrix,[25,26,27,28,29,30,31,32,33],0)\n",
    "emission_matrix=np.delete(emission_matrix,[25,26,27,28,29,30,31,32,33],1)\n",
    "initial_state=np.delete(initial_state,[25,26,27,28,29,30,31,32,33],0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8YTUTHE70GIS",
    "outputId": "d916ce83-de49-4166-9e51-e748aa703123"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(26, 2186)"
      ]
     },
     "execution_count": 266,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emission_matrix.T.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Za_L4ZQI0GIT"
   },
   "outputs": [],
   "source": [
    "def viterbi(words,V, a, b, initial_distribution):\n",
    "    T = V.shape[0]\n",
    "    M = a.shape[0]\n",
    " \n",
    "    omega = np.zeros((T, M))\n",
    "    counter=0\n",
    "    for i in np.unique(words):\n",
    "        if(V[0]==i or \"'\"+V[0]==i or \"'\"+V[0]+\"'\"==i ):\n",
    "            initial_obs=counter\n",
    "            print(\"initial_obs\")\n",
    "        counter+=1\n",
    "    omega[0, :] = np.log(initial_distribution * b[:, initial_obs])\n",
    " \n",
    "    prev = np.zeros((T - 1, M))\n",
    " \n",
    "    for t in range(1, T):\n",
    "        for j in range(M):\n",
    "            counter=0\n",
    "            for i in np.unique(words):\n",
    "                if(V[t]==i):\n",
    "                    break\n",
    "                counter+=1\n",
    "            # Same as Forward Probability\n",
    "            probability = omega[t - 1] + np.log(a[:, j]) + np.log(b[j, counter])\n",
    " \n",
    "            # This is our most probable state given previous state at time t (1)\n",
    "            prev[t - 1, j] = np.argmax(probability)\n",
    " \n",
    "            # This is the probability of the most probable state (2)\n",
    "            omega[t, j] = np.max(probability)\n",
    " \n",
    "    # Path Array\n",
    "    S = np.zeros(T)\n",
    " \n",
    "    # Find the most probable last hidden state\n",
    "    last_state = np.argmax(omega[T - 1, :])\n",
    " \n",
    "    S[0] = last_state\n",
    " \n",
    "    backtrack_index = 1\n",
    "    for i in range(T - 2, -1, -1):\n",
    "        S[backtrack_index] = prev[i, int(last_state)]\n",
    "        last_state = prev[i, int(last_state)]\n",
    "        backtrack_index += 1\n",
    " \n",
    "    # Flip the path array since we were backtracking\n",
    "    S = np.flip(S, axis=0)\n",
    " \n",
    "    # Convert numeric values to actual hidden states\n",
    "    result = []\n",
    "    \n",
    "    for s in S:\n",
    "        if s == 0:\n",
    "            result.append(\"NN\")\n",
    "        elif s==1:\n",
    "            result.append(\"JJ\")\n",
    "        elif s==2:\n",
    "            result.append(\"VFM\")\n",
    "        elif s==3:\n",
    "            result.append(\"SYM\")\n",
    "        elif s==4:\n",
    "            result.append(\"NNP\")\n",
    "        elif s==5:\n",
    "            result.append(\"NNC\")\n",
    "        elif s==6:\n",
    "            result.append(\"INTF\")\n",
    "        elif s==7:\n",
    "            result.append(\"CC\")\n",
    "        elif s==8:\n",
    "            result.append(\"PREP\")\n",
    "        elif s==9:\n",
    "            result.append(\"PRP\")\n",
    "        elif s==10:\n",
    "            result.append(\"NVB\")\n",
    "        elif s==11:\n",
    "            result.append(\"VAUX\")\n",
    "        elif s==12:\n",
    "            result.append(\"PUNC\")\n",
    "        elif s==13:\n",
    "            result.append(\"NNPC\")\n",
    "        elif s==14:\n",
    "            result.append(\"VRB\")\n",
    "        elif s==15:\n",
    "            result.append(\"VNN\")\n",
    "        elif s==16:\n",
    "            result.append(\"RB\")\n",
    "        elif s==17:\n",
    "            result.append(\"QFUNM\")\n",
    "        elif s==18:\n",
    "            result.append(\"RP\")\n",
    "        elif s==19:\n",
    "            result.append(\"NEG\")\n",
    "        elif s==20:\n",
    "            result.append(\"QF\")\n",
    "        elif s==21:\n",
    "            result.append(\"JVB\")\n",
    "        elif s==22:\n",
    "            result.append(\"NLOC\")\n",
    "        elif s==23:\n",
    "            result.append(\"VJJ\")\n",
    "        elif s==24:\n",
    "            result.append(\"QW\")\n",
    "        elif s==25:\n",
    "            result.append(\"VM\")\n",
    "        elif s==26:\n",
    "            result.append(\"JJC\")\n",
    "        elif s==27:\n",
    "            result.append(\"PSP\")\n",
    "        elif s==28:\n",
    "            result.append(\"NST\")\n",
    "        elif s==29:\n",
    "            result.append(\"QC\")\n",
    "        elif s==30:\n",
    "            result.append(\"DEM\")\n",
    "        elif s==31:\n",
    "            result.append(\"WQ\")\n",
    "        elif s==32:\n",
    "            result.append(\"QO\")\n",
    "        elif s==33:\n",
    "            result.append(\"RDP\")\n",
    "        elif s==34:\n",
    "            result.append(\"\") \n",
    "    return result\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DUCR-BgN0GIT",
    "outputId": "698cd4b7-82f8-4bf4-ffa2-ea6eaffa9428"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial_obs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\JP\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:12: RuntimeWarning: divide by zero encountered in log\n",
      "  if sys.path[0] == '':\n",
      "C:\\Users\\JP\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:24: RuntimeWarning: divide by zero encountered in log\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['NN', 'NN', 'NNP', 'SYM', 'NNC']"
      ]
     },
     "execution_count": 268,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "viterbi(list_of_hindi,np.array([\"पूर्ण\",\"प्रतिबंध\",\"हटाओ\",\":\",\"इराक\"]), transition_matrix, emission_matrix.T, initial_state)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Proyecto Final.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
